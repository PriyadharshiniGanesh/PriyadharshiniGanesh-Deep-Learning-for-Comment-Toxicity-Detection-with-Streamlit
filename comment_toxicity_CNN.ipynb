{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d06e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dense, Dropout, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8be5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = r\"C:\\Users\\vicky\\Documents\\python\\DS_ML\\ds_priya\\test.csv\"\n",
    "file_path2 = r\"C:\\Users\\vicky\\Documents\\python\\DS_ML\\ds_priya\\train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0b0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(file_path1)\n",
    "train = pd.read_csv(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07ce9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 50ms/step - loss: 0.0940\n",
      "Epoch 2/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 71ms/step - loss: 0.0536\n",
      "Epoch 3/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 47ms/step - loss: 0.0488\n",
      "Epoch 4/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 48ms/step - loss: 0.0456\n",
      "Epoch 5/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 40ms/step - loss: 0.0419\n",
      "Epoch 6/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 39ms/step - loss: 0.0399\n",
      "Epoch 7/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 39ms/step - loss: 0.0375\n",
      "Epoch 8/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 39ms/step - loss: 0.0359\n",
      "Epoch 9/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 40ms/step - loss: 0.0337\n",
      "Epoch 10/10\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 40ms/step - loss: 0.0331\n",
      "\u001b[1m4987/4987\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\W\", ' ', text)\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to each comment\n",
    "train['comment_text'] = train['comment_text'].apply(clean_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train['comment_text'])\n",
    "sequences = tokenizer.texts_to_sequences(train['comment_text'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Prepare labels\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = train[labels].values\n",
    "\n",
    "# Build the CNN model\n",
    "input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = Embedding(MAX_NB_WORDS, 128)(input_layer)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({'id': train['id']})\n",
    "submission[labels] = predictions\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
